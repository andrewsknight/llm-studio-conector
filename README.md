# Chat LLM

Una aplicaciÃ³n web minimalista y elegante para chatear con modelos de lenguaje compatibles con OpenAI, incluyendo LM Studio.

## CaracterÃ­sticas

- ğŸ¤– **Chat con streaming en tiempo real** - Respuestas que se generan en directo
- âš™ï¸ **ConfiguraciÃ³n flexible** - URL de API, clave, modelo, temperatura, tokens y prompt del sistema
- ğŸ’¾ **Persistencia local** - Conversaciones y configuraciÃ³n guardadas automÃ¡ticamente
- ğŸŒ™ **Tema oscuro/claro** - Toggle suave entre temas
- ğŸ”„ **Control de generaciÃ³n** - Capacidad de detener respuestas en curso
- ğŸ“± **DiseÃ±o responsive** - Optimizado para mÃ³vil y escritorio
- ğŸ”’ **Proxy opcional** - Para resolver problemas de CORS con Firebase Functions

## Ejecutar localmente

### Prerrequisitos

- Node.js 18+
- LM Studio (opcional) o cualquier servidor compatible con OpenAI

### InstalaciÃ³n

```bash
# Clonar e instalar dependencias
npm install

# Ejecutar en desarrollo
npm run dev
```

La aplicaciÃ³n estarÃ¡ disponible en `http://localhost:5173`

### Configurar LM Studio

1. Descargar e instalar [LM Studio](https://lmstudio.ai/)
2. Descargar un modelo (ej: Llama 3, Mistral, etc.)
3. Iniciar el servidor local:
   - Ir a "Local Server" en LM Studio
   - Cargar un modelo
   - Iniciar servidor en puerto 1234
   - âœ… Habilitar CORS si es necesario

### ConfiguraciÃ³n de la aplicaciÃ³n

1. Abrir ajustes (âš™ï¸) en la aplicaciÃ³n
2. Configurar:
   - **URL Base**: `http://localhost:1234/v1` (LM Studio)
   - **API Key**: Opcional para LM Studio, requerida para OpenAI
   - **Modelo**: Nombre del modelo cargado
   - **Temperatura**: 0-2 (creatividad de respuestas)
   - **Max Tokens**: LÃ­mite de longitud de respuesta
   - **System Prompt**: Instrucciones para el comportamiento del modelo

## Deploy en Firebase

### PreparaciÃ³n

```bash
# Instalar Firebase CLI
npm install -g firebase-tools

# Login en Firebase
firebase login

# Inicializar proyecto (si no estÃ¡ configurado)
firebase init hosting functions
```

### Deploy solo Frontend

```bash
# Construir aplicaciÃ³n
npm run build

# Deploy a Firebase Hosting
firebase deploy --only hosting
```

### Deploy con Proxy (Frontend + Functions)

El proxy es Ãºtil para resolver problemas de CORS cuando el servidor LLM no los permite.

```bash
# Instalar dependencias de functions
cd functions
npm install
cd ..

# Construir aplicaciÃ³n
npm run build

# Deploy todo
firebase deploy --only hosting,functions
```

#### Configurar variables de entorno para Functions

```bash
# Opcional: configurar URL y API Key por defecto
firebase functions:config:set api.base_url="http://localhost:1234/v1"
firebase functions:config:set api.key="tu-api-key"

# Re-deploy functions
firebase deploy --only functions
```

### URLs despuÃ©s del deploy

- **Frontend**: `https://tu-proyecto.web.app`
- **Proxy API**: `https://tu-proyecto.web.app/api/chat`

## Uso del Proxy

Cuando estÃ¡ habilitado en configuraciÃ³n, las peticiones van a travÃ©s de Firebase Functions en lugar de directamente al servidor LLM. Esto resuelve:

- âŒ Problemas de CORS
- ğŸ”’ Ocultar API keys del cliente
- ğŸŒ Acceso desde cualquier dominio

## Estructura del proyecto

```
/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/          # Componentes de UI
â”‚   â”‚   â”œâ”€â”€ ChatMessage.tsx  # Renderizado de mensajes
â”‚   â”‚   â”œâ”€â”€ ChatInput.tsx    # Input con auto-resize
â”‚   â”‚   â”œâ”€â”€ SettingsDrawer.tsx # Panel de configuraciÃ³n
â”‚   â”‚   â”œâ”€â”€ Toolbar.tsx      # Barra superior
â”‚   â”‚   â””â”€â”€ EmptyState.tsx   # Estado vacÃ­o
â”‚   â”œâ”€â”€ hooks/               # Hooks personalizados
â”‚   â”‚   â”œâ”€â”€ useChat.ts       # LÃ³gica del chat
â”‚   â”‚   â””â”€â”€ useLocalStorage.ts # Persistencia local
â”‚   â”œâ”€â”€ lib/                 # LibrerÃ­as y utilidades
â”‚   â”‚   â”œâ”€â”€ openaiClient.ts  # Cliente para APIs OpenAI
â”‚   â”‚   â”œâ”€â”€ streaming.ts     # Manejo de SSE
â”‚   â”‚   â”œâ”€â”€ types.ts         # Tipos TypeScript
â”‚   â”‚   â””â”€â”€ constants.ts     # Constantes
â”‚   â”œâ”€â”€ App.tsx             # Componente principal
â”‚   â”œâ”€â”€ main.tsx            # Punto de entrada
â”‚   â””â”€â”€ index.css           # Estilos globales
â”œâ”€â”€ functions/              # Firebase Functions (proxy)
â”œâ”€â”€ public/                # Archivos estÃ¡ticos
â””â”€â”€ dist/                  # Build de producciÃ³n
```

## Seguridad

âš ï¸ **Importante**: Nunca expongas API keys en el cÃ³digo del cliente. Opciones seguras:

1. **LM Studio local**: No requiere API key
2. **Proxy con Functions**: API key en variables de entorno del servidor
3. **Variables de entorno**: Solo para desarrollo local

## SoluciÃ³n de problemas

### Error de CORS

```bash
# OpciÃ³n 1: Habilitar proxy en configuraciÃ³n de la app
# OpciÃ³n 2: Ejecutar LM Studio con CORS habilitado
# OpciÃ³n 3: Usar extensiÃ³n de navegador para CORS (solo desarrollo)
```

### LM Studio no conecta

1. âœ… Verificar que el servidor estÃ© iniciado
2. âœ… Comprobar que el puerto sea 1234
3. âœ… Verificar la URL: `http://localhost:1234/v1`
4. âœ… Probar endpoint: `curl http://localhost:1234/v1/models`

### Errores de build

```bash
# Limpiar dependencias y reinstalar
rm -rf node_modules package-lock.json
npm install

# Verificar versiones de Node/npm
node --version
npm --version
```

## Scripts disponibles

- `npm run dev` - Servidor de desarrollo
- `npm run build` - Construir para producciÃ³n
- `npm run preview` - Preview del build
- `npm run lint` - Verificar cÃ³digo
- `npm run format` - Formatear cÃ³digo

## TecnologÃ­as utilizadas

- **Frontend**: React 18 + TypeScript + Vite
- **Estilos**: TailwindCSS con sistema de colores personalizado
- **Markdown**: marked + DOMPurify para sanitizaciÃ³n
- **Backend**: Firebase Functions (proxy opcional)
- **Persistencia**: localStorage del navegador

## Licencia

MIT# llm-studio-conector
